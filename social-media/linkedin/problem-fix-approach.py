# ---
# jupyter:
#   jupytext:
#     formats: py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.16.4
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# # LinkedIn posting

# %% [markdown]
# ## Audience problem

# %% [markdown]
# ## Approach to fix

# %% [markdown]
# ## Code or approach to fix

# %% [markdown]
# # 08/26 - 08/30

# %% [markdown]
# ## 2024-08-26
#
# Data engineering roles can vary greatly, even among companies of similar size. This considerable variance can make it incredibly difficult to identify critical improvement areas.
#
#
# There is a better way to ensure that you continue to level up without all the uncertainty.
#
#
# I recommend approaching this in 2 ways:
#
# 1. Domain expertise: Know your data, what process generates the data, what its caveats are, how it is being used, etc. If you become familiar with an industry (e.g., commerce, marketing, etc.), the expertise will be carried over to companies in the same domain.
#
# 2. Tech skills: Know fundamentals of SQL, Python, SWE best practices, CI/CD, and Distributed data processing foundations.
#
# These will take you far! Focus on these, and you will be well on your way to accelerated career growth!
#
# #dataengineering
# #data
# #datajobs
# #careerdevelopment

# %% [markdown]
# ## 2024-08-27
#
# Figuring out what to learn to be able to “data engineer” is sometimes more difficult than the actual job!
#
# Instead of trying to learn everything that are posted in a job  description, a better way would be to have the foundations locked down.
#
# Check out this practical DE roadmap that will give you the confidence to dig deep into your area of interest:
#
# 1. SQL: Basics and advanced. Use this repo to learn the basics and advanced topics: https://github.com/josephmachado/adv_data_transformation_in_sql.
#
# 2. Python: Basics (data structures, control flow) and how to extract,  transform, and load data. Use this repo to learn basics and ETL in  Python: https://github.com/josephmachado/python_essentials_for_data_engineers
#
# 3. Building data pipelines and orchestrating: Play around with an  Airflow DAG, see how the code corresponds to the DAG, how environment variables are set, and how data is transformed. Use this post to get  started: https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/
#
# 4. Distributed data processing fundamentals: Dig into how data is processed in parallel and how storage is optimized for read patterns. Use  this course to dig into Spark and its implementation details: https://josephmachado.podia.com/efficient-data-processing-in-spark
#
# Put them together following principles here https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/
#
# #data #dataengineering #dataskills

# %% [markdown]
# ## 2024-08-28
#
# Diving head-first into designing and building pipelines is usually a recipe for disaster!
#
# If you are struggling to design a pipeline (or any SWE) system, it's mostly due to not having all the required data points.
#
# Here are three categories of questions you need to know the answer to when building a data system
#
# 1. Clear requirements: You have to, in great detail know
#
#        * Who is going to use your data
#
#        * Why are they using your data
#
#        * How do they plan to use the data
#
#        * What is the context in which your data will be used
#
#        * More context-specific details 
#
# 2. Input data: You should know, at the minimum, the following:
#
#         * What does the data represent
#
#         * What is the business process that generates the data
#
#         * Logical model and ERD
#
#         * Data size and throughput, etc 
#
# 3. Existing systems: What existing systems can you re-use? Do not spin up a new platform if your existing system is sufficient
#
#
# #data
# #dataengineering
# #datapipeline

# %% [markdown]
# ## 2024-08-29
#
# Are you a data analyst looking to break into data engineering? You are already in a good position to transition.
#
# But there will probably need to be work done in the engineering part. 
#
# Do this:
#
# You might already be using SQL to pull data from a data warehouse and Python for analysis. You can start by
#
# 1. Automating one data pull using Python.
#
# 2. Scheduling that data pull to run at a specific time every day using cron.
#
# 3. Automating more data pulls. For complex data pulls, setup Airflow. You can try it here sample airflow project .
#
# 4. Understand your data warehouse infrastructure. (e.g. size of the warehouse cluster, partitions, how data is loaded etc).
#
# 5. If you can, do some of the data processing in Apache Spark using AWS EMR or in GCP dataflow. This would be great.
#
# 6. As a data analyst/scientist, you have a very clear understanding of the data, where it comes from, how it is used, its nuances, etc. Use this knowledge to build or advise a better data processing pipeline.
#
# 7. You can also create a data dictionary for your company. While this  may not be technical, this will prove extremely valuable to your  company, build trust and you will be able to see patterns in the data  pipeline that you can improve.
#
# 8. Ask for it, It is usually difficult to get cross team work but you  won’t know unless you try. A very important skill to have is to manage up, i.e letting your manager know what work you want to do and coming up with a selling point that adds value to the company.
#
# 9. Leetcode is crucial for interviews as well. Check out https://www.startdataengineering.com/post/de_interview_dsa/ 
#
# Also remember the market is rough rn, so give yourself time!

# %% [markdown]
# ## 2024-08-30

# %% [markdown]
#

# %% [markdown]
#
